{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from charity import Charity\n",
    "\n",
    "tsv_path = 'data/wikipedia.tsv'\n",
    "\n",
    "with open(tsv_path, 'r') as tsv_file:\n",
    "    reader = csv.DictReader(tsv_file, dialect='excel-tab')\n",
    "    charities = [\n",
    "        Charity(**row)\n",
    "        for row in reader\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [charity.description for charity in charities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bert import get_embeddings\n",
    "\n",
    "embeddings_index = []\n",
    "embeddings_list = []\n",
    "\n",
    "for doc_index, doc in enumerate(documents):\n",
    "    print(doc_index)\n",
    "    sentences = [\n",
    "        re.sub('\\[\\d+\\]', '', sent).strip().lower()\n",
    "        for sent in sent_tokenize(doc)\n",
    "    ]\n",
    "    sentence_embeddings = get_embeddings(sentences)\n",
    "    \n",
    "    embeddings_index += ([doc_index] * len(sentence_embeddings))\n",
    "    embeddings_list.append(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "embeddings = torch.cat(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sample_indices(index, n_positive, n_negative, n_examples):\n",
    "    all_query_examples = []\n",
    "    all_positive_examples = []\n",
    "    all_negative_examples = []\n",
    "    \n",
    "    unique_elements = np.unique(index)\n",
    "    index_indices = torch.arange(len(index))\n",
    "    \n",
    "    for _ in range(n_examples):\n",
    "        elem = random.choice(unique_elements)\n",
    "        \n",
    "        elem_indices = index_indices[index == elem]\n",
    "        query_index = random.choice(elem_indices)\n",
    "        \n",
    "        same_document_indices = index_indices[(index == elem)]\n",
    "        different_document_indices = index_indices[index != elem]\n",
    "        \n",
    "        positive_examples = same_document_indices[random.sample(range(0, len(same_document_indices)), n_positive)]\n",
    "        negative_examples = different_document_indices[random.sample(range(0, len(different_document_indices)), n_negative)]\n",
    "\n",
    "        all_query_examples.append(query_index)\n",
    "        all_positive_examples.append(positive_examples)\n",
    "        all_negative_examples.append(negative_examples)\n",
    "\n",
    "    query_indices = torch.tensor(all_query_examples)\n",
    "    positive_example_indices = torch.stack(all_positive_examples)\n",
    "    negative_example_indices = torch.stack(all_negative_examples)\n",
    "    \n",
    "    return query_indices, positive_example_indices, negative_example_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# model\n",
    "input_size = 768\n",
    "output_size = 10\n",
    "linear_model = nn.Linear(input_size, output_size)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear_model.weight)\n",
    "linear_model.bias.data.fill_(0.1)\n",
    "    \n",
    "loss_function = torch.nn.MarginRankingLoss(margin=1)\n",
    "optimizer = torch.optim.Adam(linear_model.parameters(), lr=0.1)\n",
    "cos = nn.CosineSimilarity(dim=2, eps=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_now = False\n",
    "\n",
    "if shuffle_now:\n",
    "    shuffled_indices = np.arange(len(embeddings_index))\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(embeddings, embeddings_index, n_positive, n_negative, n_examples):\n",
    "    query_indices, positive_example_indices, negative_example_indices = sample_indices(\n",
    "        embeddings_index,\n",
    "        n_positive=n_positive,\n",
    "        n_negative=n_negative,\n",
    "        n_examples=n_examples,\n",
    "    )\n",
    "    \n",
    "    model_embeddings = linear_model(embeddings)\n",
    "    embedded_query = model_embeddings[query_indices]\n",
    "    embedded_positive = model_embeddings[positive_example_indices]\n",
    "    embedded_negative = model_embeddings[negative_example_indices]\n",
    "    \n",
    "    positive_similarity = cos(embedded_query.view(-1, 1, output_size), embedded_positive)    \n",
    "    negative_similarity = cos(embedded_query.view(-1, 1, output_size), embedded_negative)\n",
    "\n",
    "    positive_flat = positive_similarity.view(-1, 1).repeat(1, n_negative).view(-1)\n",
    "    negative_flat = negative_similarity.repeat(1, n_positive).view(-1)\n",
    "    \n",
    "    all_ones = torch.ones(len(positive_flat))\n",
    "\n",
    "    return loss_function(positive_flat, negative_flat, all_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5000\n",
    "n_positive = 1\n",
    "n_negative = 10\n",
    "\n",
    "train_split = 0.6\n",
    "split_index = int(len(shuffled_indices) * train_split)\n",
    "train_indices = shuffled_indices[:split_index]\n",
    "validation_indices = shuffled_indices[split_index:]\n",
    "\n",
    "train_embeddings_index = torch.tensor(embeddings_index)[train_indices]\n",
    "validation_embeddings_index = torch.tensor(embeddings_index)[validation_indices]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_loss = compute_loss(embeddings, train_embeddings_index, n_positive, n_negative, n_examples)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        validation_loss = compute_loss(\n",
    "            embeddings,\n",
    "            validation_embeddings_index,\n",
    "            n_positive,\n",
    "            n_negative,\n",
    "            n_examples,\n",
    "        )\n",
    "    \n",
    "    print('epoch: {} train loss: {} validation loss: {}'.format(\n",
    "        epoch,\n",
    "        train_loss,\n",
    "        validation_loss,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "search = \"dogs\"\n",
    "\n",
    "cos_out = nn.CosineSimilarity(dim=1, eps=1e6)\n",
    "\n",
    "embedded = get_embeddings([search])\n",
    "processed = linear_model(embedded)\n",
    "processed_embeddings = linear_model(embeddings)\n",
    "\n",
    "similarities = cosine_similarity(\n",
    "    processed.view(-1, output_size),\n",
    "    processed_embeddings,\n",
    "    dim=1)\n",
    "\n",
    "top_n = similarities.argsort().tolist()[::-1][:5]\n",
    "doc_indices = torch.tensor(embeddings_index)[top_n]\n",
    "[charities[i].name for i in doc_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([336, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([336, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
